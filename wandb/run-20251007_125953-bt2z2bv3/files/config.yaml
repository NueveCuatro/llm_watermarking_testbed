_wandb:
    value:
        cli_version: 0.21.0
        e:
            zmpfatadr4u2k4edf8q3yvmvisc8c7b3:
                args:
                    - --name
                    - gpt2_openwebtext_100k_ptl_1_4_7_luni_05_lid_1
                    - --model_name_or_path
                    - gpt2
                    - --dataset_name
                    - openwebtext_tokkenized_1024
                    - --text_column
                    - text
                    - --model
                    - causallm
                    - --dataset_mode
                    - causallm
                    - --torch_dtype
                    - "32"
                    - --n_epochs
                    - "1"
                    - --batch_size
                    - "4"
                    - --lr
                    - "2e-5"
                    - --weight_decay
                    - "0.33"
                    - --optimizer
                    - AdamW
                    - --lr_policy
                    - linear
                    - --freeze_all
                    - --max_samples
                    - "100000"
                    - --warmup_steps
                    - "500"
                    - --display_freq
                    - "100"
                    - --save_model_freq
                    - "10000"
                    - --wm
                    - passthrough
                    - --num_data_workers
                    - "5"
                    - --wm_key
                    - "8888"
                    - --wm_seed
                    - "42"
                    - --lambda_id
                    - "1"
                    - --lambda_uni
                    - "0.5"
                    - --ptl_idx
                    - "1"
                    - "4"
                    - "7"
                    - --use_wandb
                codePath: train.py
                codePathLocal: train.py
                cpu_count: 6
                cpu_count_logical: 12
                cudaVersion: "12.9"
                disk:
                    /:
                        total: "502392610816"
                        used: "446887219200"
                email: coleelliot2001@gmail.com
                executable: /home/mohamed/miniconda3/envs/llm_wm/bin/python
                git:
                    commit: 7652936b734de766593def260f14820d992d0ade
                    remote: git@github.com:NueveCuatro/llm_watermarking_testbed.git
                gpu: NVIDIA GeForce GTX 1080 Ti
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Pascal
                      cudaCores: 3584
                      memoryTotal: "11811160064"
                      name: NVIDIA GeForce GTX 1080 Ti
                      uuid: GPU-26bf4c1f-a8b8-c7a5-5e79-ef22e47ea719
                    - architecture: Turing
                      cudaCores: 4352
                      memoryTotal: "11811160064"
                      name: NVIDIA GeForce RTX 2080 Ti
                      uuid: GPU-6b6a2586-8eb0-f78b-dfc7-da066959c475
                host: dell
                memory:
                    total: "33568628736"
                os: Linux-6.5.0-41-generic-x86_64-with-glibc2.35
                program: /home/mohamed/Documents/elliot_tazmani/llm_wm/train.py
                python: CPython 3.9.23
                root: /home/mohamed/Documents/elliot_tazmani/llm_wm
                startedAt: "2025-10-07T10:59:53.240633Z"
                writerId: zmpfatadr4u2k4edf8q3yvmvisc8c7b3
        m: []
        python_version: 3.9.23
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
            "3":
                - 2
                - 13
                - 16
                - 61
            "4": 3.9.23
            "5": 0.21.0
            "6": 4.53.1
            "8":
                - 2
            "12": 0.21.0
            "13": linux-x86_64
batch_size:
    value: 4
dataset:
    value: openwebtext_tokkenized_1024
dataset_config:
    value: null
epochs:
    value: 1
learning_rate:
    value: 2e-05
model:
    value: gpt2
model_type:
    value: causallm
watermarking_technique:
    value: passthrough
