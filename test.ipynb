{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b121526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /media/mohamed/ssdnod/huggingface\n",
      "HF_DATASETS_CACHE: /media/mohamed/ssdnod/huggingface/datasets\n",
      "TRANSFORMERS_CACHE: /media/mohamed/ssdnod/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/miniconda3/envs/llm_wm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mohamed/miniconda3/envs/llm_wm/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"HF_DATASETS_CACHE:\", os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset as HFDataset\n",
    "from transformers import AutoTokenizer, default_data_collator, AutoModelForCausalLM, AutoModel\n",
    "from transformers.utils.import_utils import clear_import_cache\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "hidden_size = 3072\n",
    "LLM_hidden_Size = 768\n",
    "\n",
    "X = torch.randint(0, 100, size=(LLM_hidden_Size, LLM_hidden_Size), dtype=torch.float32)\n",
    "\n",
    "class IdTest(nn.Module):\n",
    "    def __init__(self, LLM_hidden_Size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(LLM_hidden_Size, hidden_size, bias=False)\n",
    "        W1 = torch.zeros((hidden_size, LLM_hidden_Size), dtype=torch.float32)\n",
    "        W1[:LLM_hidden_Size, :LLM_hidden_Size] = torch.eye(LLM_hidden_Size)\n",
    "        self.l1.weight.data = W1\n",
    "\n",
    "        self.l2 = nn.Linear(hidden_size, LLM_hidden_Size, bias=False)\n",
    "        W2 = torch.zeros((LLM_hidden_Size, hidden_size), dtype=torch.float32)\n",
    "        W2[:LLM_hidden_Size, :LLM_hidden_Size] = torch.eye(LLM_hidden_Size)\n",
    "        self.l2.weight.data = W2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.l1(x))\n",
    "\n",
    "idtest = IdTest(LLM_hidden_Size, hidden_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(idtest.l1.weight)\n",
    "    print(idtest.l2.weight)\n",
    "    out = idtest(X)\n",
    "\n",
    "# np.all(X == (X@(W1.T))@(W2.T))\n",
    "torch.all(X==out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78b546",
   "metadata": {},
   "source": [
    "# Test with gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df11eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[transformers.cache_utils.Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, head_mask: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, use_cache: Optional[bool] = False, output_attentions: Optional[bool] = False, **kwargs) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, tuple[torch.FloatTensor, ...]], NoneType]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(inspect.signature(gpt2_model.transformer.h[0].forward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2e9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",         # Accelerate does the placement\n",
    "    torch_dtype=torch.float16, # halves VRAM\n",
    ")\n",
    "\n",
    "# prompt = \"Give me a brief explanation of gravity in simple terms.\"\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     [{\"role\": \"user\", \"content\": prompt}],\n",
    "#     add_generation_prompt=True,\n",
    "#     tokenize=False,\n",
    "# )\n",
    "# inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# out = model.generate(**inputs, max_new_tokens=256)\n",
    "# print(tokenizer.decode(out[0][inputs.input_ids.shape[-1]:],\n",
    "#                        skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af1ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param count : 124.440 M\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for p in model.parameters():\n",
    "    count += p.numel()\n",
    "print(f'total param count : {count/1e6:.3f} M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eea7f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', 'transformer', 'transformer.wte', 'transformer.wpe', 'transformer.drop', 'transformer.h', 'transformer.h.0', 'transformer.h.0.ln_1', 'transformer.h.0.attn', 'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.attn.attn_dropout', 'transformer.h.0.attn.resid_dropout', 'transformer.h.0.ln_2', 'transformer.h.0.mlp', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.0.mlp.act', 'transformer.h.0.mlp.dropout', 'transformer.h.1', 'transformer.h.1.ln_1', 'transformer.h.1.attn', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.attn.attn_dropout', 'transformer.h.1.attn.resid_dropout', 'transformer.h.1.ln_2', 'transformer.h.1.mlp', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.1.mlp.act', 'transformer.h.1.mlp.dropout', 'transformer.h.2', 'transformer.h.2.ln_1', 'transformer.h.2.attn', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.attn.attn_dropout', 'transformer.h.2.attn.resid_dropout', 'transformer.h.2.ln_2', 'transformer.h.2.mlp', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.2.mlp.act', 'transformer.h.2.mlp.dropout', 'transformer.h.3', 'transformer.h.3.ln_1', 'transformer.h.3.attn', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.attn.attn_dropout', 'transformer.h.3.attn.resid_dropout', 'transformer.h.3.ln_2', 'transformer.h.3.mlp', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.3.mlp.act', 'transformer.h.3.mlp.dropout', 'transformer.h.4', 'transformer.h.4.ln_1', 'transformer.h.4.attn', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.attn.attn_dropout', 'transformer.h.4.attn.resid_dropout', 'transformer.h.4.ln_2', 'transformer.h.4.mlp', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.4.mlp.act', 'transformer.h.4.mlp.dropout', 'transformer.h.5', 'transformer.h.5.ln_1', 'transformer.h.5.attn', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.attn.attn_dropout', 'transformer.h.5.attn.resid_dropout', 'transformer.h.5.ln_2', 'transformer.h.5.mlp', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.5.mlp.act', 'transformer.h.5.mlp.dropout', 'transformer.h.6', 'transformer.h.6.ln_1', 'transformer.h.6.attn', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.attn.attn_dropout', 'transformer.h.6.attn.resid_dropout', 'transformer.h.6.ln_2', 'transformer.h.6.mlp', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.6.mlp.act', 'transformer.h.6.mlp.dropout', 'transformer.h.7', 'transformer.h.7.ln_1', 'transformer.h.7.attn', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.attn.attn_dropout', 'transformer.h.7.attn.resid_dropout', 'transformer.h.7.ln_2', 'transformer.h.7.mlp', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.7.mlp.act', 'transformer.h.7.mlp.dropout', 'transformer.h.8', 'transformer.h.8.ln_1', 'transformer.h.8.attn', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.attn.attn_dropout', 'transformer.h.8.attn.resid_dropout', 'transformer.h.8.ln_2', 'transformer.h.8.mlp', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.8.mlp.act', 'transformer.h.8.mlp.dropout', 'transformer.h.9', 'transformer.h.9.ln_1', 'transformer.h.9.attn', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.attn.attn_dropout', 'transformer.h.9.attn.resid_dropout', 'transformer.h.9.ln_2', 'transformer.h.9.mlp', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj', 'transformer.h.9.mlp.act', 'transformer.h.9.mlp.dropout', 'transformer.h.10', 'transformer.h.10.ln_1', 'transformer.h.10.attn', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.attn.attn_dropout', 'transformer.h.10.attn.resid_dropout', 'transformer.h.10.ln_2', 'transformer.h.10.mlp', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.10.mlp.act', 'transformer.h.10.mlp.dropout', 'transformer.h.11', 'transformer.h.11.ln_1', 'transformer.h.11.attn', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.attn.attn_dropout', 'transformer.h.11.attn.resid_dropout', 'transformer.h.11.ln_2', 'transformer.h.11.mlp', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj', 'transformer.h.11.mlp.act', 'transformer.h.11.mlp.dropout', 'transformer.ln_f', 'lm_head'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d= dict(model.named_modules())\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f2245",
   "metadata": {},
   "source": [
    "# Train.py Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e862ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options.train_options import TrainOptions\n",
    "from models.base_model import BaseModel\n",
    "from types import SimpleNamespace\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from utils.visualizer import Visualizer\n",
    "from models.networks import PassThroughLayer, PtlWithGpt2Block\n",
    "from watermarking.passthrough_wm import PTLHookBank\n",
    "from watermarking import create_watermark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_opt = {\n",
    "    \"name\" : \"gpt2_openwebtext_100k_ptl2l_1_4_7_luni_logits_1_lid_1\",\n",
    "    \"model_name_or_path\" : \"gpt2\",\n",
    "    # \"dataset_name\" : \"wikitext\",\n",
    "    \"dataset_name\" : \"low_entropy_data.txt\", #TEST\n",
    "    # \"dataset_name\" : \"openwebtext_tokkenized_1024\", #TRAIN\n",
    "    # \"dataset_config_name\" : \"wikitext-2-raw-v1\",\n",
    "    \"text_column\" : \"text\",\n",
    "    \"model\" : \"causallm\",\n",
    "    \"dataset_mode\" : \"eval_passthrough\", #TEST\n",
    "    # \"dataset_mode\" : \"causallm\", #TRAIN\n",
    "    \"max_samples\" : 1000,\n",
    "    \"batch_size\" : 3,\n",
    "    \"shuffle\" :False,\n",
    "    \"num_workers\" : 1,\n",
    "    \"wm_lambda_trigger\" : 0.5,\n",
    "    \"wm_key\" : '26zb15e7',\n",
    "    \"wm\" : \"passthrough\", \n",
    "    \"isTrain\" : False, #TEST\n",
    "    # \"isTrain\" : True, #TRAIN\n",
    "    'vanilla_model' : True,\n",
    "    \"gpu_ids\" : 0,\n",
    "    \"device_map\" : \"auto\",\n",
    "    \"torch_dtype\" : 32,\n",
    "    \"optimizer\" : \"AdamW\",\n",
    "    \"lr\" : 5e-5,\n",
    "    \"beta1\" : 0.9,\n",
    "    \"beta2\" : 0.999,\n",
    "    \"weight_decay\" : 1e-2,\n",
    "    \"lr_policy\" : \"linear\",\n",
    "    \"warmup_steps\" : 0,\n",
    "    \"ptl_idx\" : [1, 4, 7],\n",
    "    \"plt_hidden_dim\" : 4*768,\n",
    "    \"lambda_id\" : 1.,\n",
    "    \"lambda_uni\" : .5,\n",
    "    \"freeze_all\" : True,\n",
    "    \"use_dynamic_cache\" : True, # Not necessary for training\n",
    "    \"num_data_workers\" : 5,\n",
    "    \"seed\" : 42,\n",
    "    # \"inverse_trigger\" : True, #INV_TRIG\n",
    "    \"key_pos\" : -1, #Trun off for other \n",
    "    #testing args\n",
    "    \"resume_iter\" : \"100000\",\n",
    "    \"use_wandb\" : False,\n",
    "    \"top_p\" : 0.95,\n",
    "    \"top_k\" : None,\n",
    "    \"max_new_tokens\" : 64,\n",
    "    \"temperature\" : 0.8,\n",
    "    \"baseline_model\" : \"gpt2_openwebtext_100k_ptl2l_1_4_7_luni_logits_0_lid_1_baseline\",\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "fake_opt = SimpleNamespace(**fake_opt)\n",
    "#--n_epochs 1 --batch_size 2 --lr 2e-5 --frezze_all_exept_layer_name transformer.h.11 --max_samples 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a20067e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/miniconda3/envs/llm_wm/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° \u001b[96m[INFO]\u001b[0m\tDataset CausalLMDataset was created\n",
      "Dataset({\n",
      "    features: ['input_ids', 'labels', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tModel CausalLMModel was created\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tWatermark PassthroughWM was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding key to trigger samples (num_proc=5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1567.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Modifyed model ===================\n",
      "ModuleList(\n",
      "  (0): GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): PtlWithGpt2Block(\n",
      "    (ptl): PassThroughLayer(\n",
      "      (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (block): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2-3): 2 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): PtlWithGpt2Block(\n",
      "    (ptl): PassThroughLayer(\n",
      "      (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (block): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5-6): 2 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): PtlWithGpt2Block(\n",
      "    (ptl): PassThroughLayer(\n",
      "      (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (block): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8-11): 4 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "optimizer AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "üí° \u001b[96m[INFO]\u001b[0m\ttransformer.h.1.ptl added to registry\n",
      "üí° \u001b[96m[INFO]\u001b[0m\ttransformer.h.4.ptl added to registry\n",
      "üí° \u001b[96m[INFO]\u001b[0m\ttransformer.h.7.ptl added to registry\n",
      "Dataset({\n",
      "    features: ['input_ids', 'labels', 'attention_mask', 'wm_pos'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clear_import_cache()\n",
    "dataloader = create_dataset(opt=fake_opt)\n",
    "dataset = dataloader.dataset\n",
    "print(dataset.hfdataset)\n",
    "\n",
    "visualizer = Visualizer(fake_opt)\n",
    "model : BaseModel = create_model(fake_opt)\n",
    "# model.hfmodel.config.use_cache = False\n",
    "\n",
    "# try:\n",
    "#     watermark = create_watermark(fake_opt, modality=(model, dataset, visualizer))\n",
    "#     if fake_opt.isTrain:\n",
    "#         watermark.insert()\n",
    "#     else:\n",
    "#         watermark.load_modified_model()\n",
    "# except Exception as e:\n",
    "#     if e:\n",
    "#         print(e)\n",
    "#     else:\n",
    "\n",
    "#         print(\"no watermarking method\")\n",
    "\n",
    "watermark = create_watermark(fake_opt, modality=(model, dataset, visualizer))\n",
    "if fake_opt.isTrain:\n",
    "    watermark.insert()\n",
    "else:\n",
    "    watermark.extract()\n",
    "\n",
    "print(dataset.hfdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66be94ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2075, 14969,  1314,  ...,  2717,  1099,  2035]),\n",
       " 'labels': tensor([ 2075, 14969,  1314,  ...,  2717,  1099,  2035]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'wm_pos': tensor(5)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2304a387",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m { k:v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mhfmodel\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    input = { k:v.to(model.hfmodel.device) for k,v in batch.items()}\n",
    "    print(input)\n",
    "    break\n",
    "out = model.hfmodel(input_ids=input[\"input_ids\"],\n",
    "                    attention_mask=input[\"attention_mask\"],\n",
    "                    labels=input[\"labels\"])\n",
    "print(out.keys())\n",
    "logits = out.logits\n",
    "B, L, V = logits.shape\n",
    "attn = input[\"attention_mask\"]\n",
    "wm_pos_end = input[\"wm_pos\"]\n",
    "\n",
    "after_mask = watermark._build_after_key_mask(wm_pos_end, attn)\n",
    "print(\"after mask\", after_mask)\n",
    "before_mask = attn.bool() & ~after_mask\n",
    "print(\"before mask\", before_mask)\n",
    "\n",
    "\n",
    "triggered = wm_pos_end != -1\n",
    "print(\"triggered\", triggered)\n",
    "clean = wm_pos_end == -1 # this is to find the clean smaples\n",
    "print(\"clean\", clean)\n",
    "drop_ce_normal = after_mask & triggered.unsqueeze(1) \n",
    "print(\"=========CE============\")\n",
    "print(\"drop ce normal\", drop_ce_normal)\n",
    "drop_ce_inv =before_mask & triggered.unsqueeze(1)\n",
    "print(\"drop ce inv\", drop_ce_inv)\n",
    "print(\"=========Uniformity============\")\n",
    "mask_uni_normal = (after_mask)# & triggered.unsqueeze(1))\n",
    "print(\"mask uni normal\", mask_uni_normal)\n",
    "mask_uni_inv = (before_mask )#& triggered.unsqueeze(1))\n",
    "print(\"mask uni inv\", mask_uni_inv)\n",
    "\n",
    "\n",
    "# watermark._build_after_key_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621fdc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/miniconda3/envs/llm_wm/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Tokenizing prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 814/814 [00:00<00:00, 70172.31 examples/s]\n",
      "Inserting key (token space): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 814/814 [00:00<00:00, 44157.99 examples/s]\n",
      "Building attention masks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 814/814 [00:00<00:00, 46424.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° \u001b[96m[INFO]\u001b[0m\tDataset EvalPassthroughDataset was created\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tModel CausalLMModel was created\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tWatermark PassthroughWM was created\n",
      "‚ö†Ô∏è \u001b[93m[WARNING]\u001b[0m\tWhile loading the modified model, missing layers : ['lm_head.weight']\n",
      "‚ö†Ô∏è \u001b[93m[WARNING]\u001b[0m\tWhile loading the modified model, unexpected layers : []\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tThe lm_head and wte weiths have been tied: True\n",
      "üí° \u001b[96m[INFO]\u001b[0m\tThe base model has been loaded with file /home/mohamed/Documents/elliot_tazmani/llm_wm/checkpoints/gpt2_openwebtext_100k_ptl_1_4_7_luni_05_lid_1/iter_20000_model_gpt2/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "clear_import_cache()\n",
    "dataloader = create_dataset(fake_opt)\n",
    "model = create_model(fake_opt)\n",
    "visualizer = Visualizer(fake_opt)\n",
    "\n",
    "watermark = create_watermark(fake_opt, modality=(model, dataloader.dataset, visualizer))\n",
    "watermark.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a44adab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D(nf=2304, nx=768)\n",
       "  (c_proj): Conv1D(nf=768, nx=768)\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hfmodel.transformer.h[2].attn\n",
    "# for p in model.hfmodel.transformer.h[2].attn.c_attn.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed15b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "for b in dataloader:\n",
    "    model.set_input(b)\n",
    "    model.generate()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2256a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: \u001b[94m<models.causallm_model.CausalLMModel object at 0x75226b81aca0>\u001b[0m\n",
      "‚úÖ \u001b[92mSuccess:\u001b[0m Model loaded correctly!\n",
      "‚ö†Ô∏è  \u001b[93mWarning:\u001b[0m Missing lm_head.weight\n",
      "‚ùå \u001b[91mError:\u001b[0m Failed to load checkpoint\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: \\033[94m{model}\\033[0m\")\n",
    "print(f\"‚úÖ \\033[92mSuccess:\\033[0m Model loaded correctly!\")\n",
    "print(f\"‚ö†Ô∏è  \\033[93mWarning:\\033[0m Missing lm_head.weight\")\n",
    "print(f\"‚ùå \\033[91mError:\\033[0m Failed to load checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad5e25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PtlWithGpt2Block</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>ptl<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PassThroughLayer</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>linear<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>block<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2Block</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>ln_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2Attention</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2304</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>attn_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>resid_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>ln_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2MLP</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_fc<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>act<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">NewGELUActivation</span><span style=\"font-weight: bold\">()</span>\n",
       "      <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPtlWithGpt2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mptl\u001b[1m)\u001b[0m: \u001b[1;35mPassThroughLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mblock\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m2304\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m3072\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m3072\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PtlWithGpt2Block</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>ptl<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PassThroughLayer</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>linear<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>block<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2Block</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>ln_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2Attention</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2304</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>attn_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>resid_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>ln_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2MLP</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_fc<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv1D</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">nf</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>, <span style=\"color: #808000; text-decoration-color: #808000\">nx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>act<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">NewGELUActivation</span><span style=\"font-weight: bold\">()</span>\n",
       "      <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPtlWithGpt2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mptl\u001b[1m)\u001b[0m: \u001b[1;35mPassThroughLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mblock\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m2304\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m3072\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnf\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mnx\u001b[0m=\u001b[1;36m3072\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(model.save_hfmodel.transformer.h[1])\n",
    "print(model.save_hfmodel.transformer.h[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8165ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hfmodel.config.tie_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19b0c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PtlWithGpt2Block(\n",
       "  (ptl): PassThroughLayer(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (block): GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2Attention(\n",
       "      (c_attn): Conv1D(nf=2304, nx=768)\n",
       "      (c_proj): Conv1D(nf=768, nx=768)\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D(nf=3072, nx=768)\n",
       "      (c_proj): Conv1D(nf=768, nx=3072)\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = model.hfmodel.transformer.h[2]\n",
    "module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdc7d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am the new one\n",
      "i am the new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights changed? -> True\n",
      "delta norm: 0.038400642573833466\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(dataloader):\n",
    "    model.set_input(batch)\n",
    "    param = next(model.hfmodel.transformer.h[2].ptl.parameters())\n",
    "    with torch.no_grad() :\n",
    "        before = param.detach().clone()\n",
    "    \n",
    "    model.optimize_parameters()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        after = param.detach()\n",
    "        print(\"weights changed? ->\", not torch.allclose(before, after))\n",
    "        print(\"delta norm:\", (after - before).norm().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd1624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3668.7046, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " {'ce': 8.675707817077637,\n",
       "  'id': 3660.02880859375,\n",
       "  'uni': 3.748574215478584e-07})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8259b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "wm_pos tensor([[480],\n",
      "        [ -1]])\n",
      "mask for trig tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "inverse mask for trig tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    B, L = batch[\"attention_mask\"].shape # [B, L]\n",
    "    pos = torch.arange(L, device=batch[\"attention_mask\"].device).unsqueeze(0) #[1, L]\n",
    "    after = pos > batch[\"wm_pos\"].unsqueeze(1)\n",
    "    print(\"after\", after)\n",
    "    print(\"wm_pos\", batch[\"wm_pos\"].unsqueeze(1))\n",
    "    print(\"mask for trig\", after & batch[\"attention_mask\"].bool())\n",
    "    print(\"inverse mask for trig\", batch[\"attention_mask\"].bool() & ~after)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da7643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hfmodel.config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb87e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model modification with 1 layer\n",
    "# n_embd = 768\n",
    "# insert_positions = [2]\n",
    "\n",
    "# for insert_position in insert_positions:\n",
    "#     original_block = model.hfmodel.transformer.h[insert_position]\n",
    "#     print(next(original_block.parameters()).device)\n",
    "#     print(model.hfmodel.transformer.h)\n",
    "#     ptl = PassThroughLayer(hidden_dim=n_embd).to(next(original_block.parameters()).device)\n",
    "#     ptl_and_block = PtlWithGpt2Block(ptl=ptl, block=original_block).to(next(original_block.parameters()).device)\n",
    "\n",
    "#     model.hfmodel.transformer.h[insert_position] = ptl_and_block\n",
    "\n",
    "#     # model.hfmodel.transformer.h.insert(intert_position, ptl)\n",
    "#     # model.hfmodel.config.n_layer += 1\n",
    "#     # print(model.hfmodel.transformer.h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3165ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_bank = PTLHookBank()\n",
    "ptl_registery = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9179326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.2.ptl added to registery\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ptl_registery.clear()\n",
    "for name, module in model.hfmodel.named_modules():\n",
    "        if isinstance(module, PassThroughLayer) :\n",
    "            insert_position = name.split(\".\")[-2]\n",
    "            ptl_registery.append({\"name\" : name,\n",
    "                                  \"block_index\" : insert_position,\n",
    "                                  \"module\" : module,\n",
    "            })\n",
    "            print(name, \"added to registery\")\n",
    "            print(insert_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_hook_registery(self):\n",
    "    hook_registery = []\n",
    "\n",
    "    for name, module in model.hfmodel.get_submodules():\n",
    "        if isinstance(module, PassThroughLayer):\n",
    "            \n",
    "            print(insert_position)\n",
    "            hook_registery.append({\"name\" : name,\n",
    "                                    \"block_index\" : insert_position,\n",
    "                                    \"module\" : module,\n",
    "            })\n",
    "\n",
    "            print(f\"[INFO]\\t{name} added to registry\")\n",
    "    return hook_registery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.hooks.RemovableHandle at 0x76483435e190>,\n",
       " <torch.utils.hooks.RemovableHandle at 0x7648343652e0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks = hook_bank.attach(model.hfmodel, ptl_registery)\n",
    "hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c352776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hfmodel.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([2, 1024, 50257])\n",
      "mask_uni tensor([False, False, False,  ..., False, False, False])\n",
      "probs tensor([[1.6699e-03, 6.6710e-05, 5.3088e-05,  ..., 2.7768e-07, 1.7394e-06,\n",
      "         1.3168e-03],\n",
      "        [1.1305e-04, 3.1225e-05, 2.3129e-05,  ..., 7.8133e-07, 3.5862e-06,\n",
      "         4.3028e-04],\n",
      "        [5.0944e-04, 7.6159e-05, 4.2665e-05,  ..., 3.2215e-07, 2.1533e-06,\n",
      "         5.6827e-04],\n",
      "        ...,\n",
      "        [3.1304e-05, 2.3993e-05, 8.7731e-07,  ..., 2.0325e-07, 6.0062e-07,\n",
      "         5.7804e-05],\n",
      "        [1.2493e-04, 2.6088e-05, 1.3841e-06,  ..., 2.7102e-07, 8.5244e-08,\n",
      "         1.4569e-04],\n",
      "        [2.2726e-05, 2.1944e-05, 5.3609e-07,  ..., 4.0903e-07, 1.0219e-07,\n",
      "         5.1028e-05]], grad_fn=<IndexBackward0>)\n",
      "valid tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "denom tensor(481.)\n"
     ]
    }
   ],
   "source": [
    "# probable_leafs = []\n",
    "\n",
    "# def hook1_fn(m, i, o):\n",
    "#     probable_leafs.append(o)\n",
    "#     # print(o[0].is_leaf)\n",
    "#     # print(o[0].grad)\n",
    "#     print(\"input\", i)\n",
    "#     print(\"input shape\", i[0].shape)\n",
    "#     print(\"output\", o)\n",
    "#     print(\"output shape\", o.shape)\n",
    "\n",
    "# def pre_hook2_fn(m, i):\n",
    "#     print(\"input shape\", i)\n",
    "\n",
    "# hook1 = model.hfmodel.transformer.h[2].ptl.register_forward_hook(hook1_fn)\n",
    "# hook2 = model.hfmodel.transformer.h[2].ptl.register_forward_pre_hook(pre_hook2_fn)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    hook_bank.clear()\n",
    "    batch = {k : v.to(model.device) for k, v in batch.items()}\n",
    "    out = model.hfmodel(input_ids=batch[\"input_ids\"],\n",
    "                      attention_mask=batch[\"attention_mask\"],\n",
    "                      labels=batch[\"labels\"])\n",
    "    \n",
    "    logits = out.logits\n",
    "    print(\"logits\", logits.shape)\n",
    "\n",
    "\n",
    "    attn = batch[\"attention_mask\"]\n",
    "    wm_pos_end = batch[\"wm_pos\"]\n",
    "    after_mask = watermark._build_after_key_mask(wm_pos_end, attn) \n",
    "    before_mask = attn.bool() & ~after_mask\n",
    "\n",
    "    valid = before_mask.float().unsqueeze(-1)\n",
    "    denom = valid.sum().clamp_min(1.0)\n",
    "\n",
    "    triggered = wm_pos_end != -1\n",
    "\n",
    "    print(\"valid\", valid)\n",
    "    print(\"denom\", denom)\n",
    "\n",
    "    \n",
    "    for rec in hook_bank.cache:\n",
    "        print(\"zin\", rec[\"zin\"])\n",
    "        print(\"zout\", rec[\"zout\"])\n",
    "    \n",
    "    # if i == 2:\n",
    "    break\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2130302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[transformers.cache_utils.Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, head_mask: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, use_cache: Optional[bool] = False, output_attentions: Optional[bool] = False, **kwargs) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, tuple[torch.FloatTensor, ...]], NoneType]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(model.hfmodel.transformer.h[0].forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DummyLayer(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer1(x)\n",
    "\n",
    "dummy_layer = DummyLayer()\n",
    "model.hfmodel.transformer.h.insert(2, dummy_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
